{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T20:04:51.639646Z","iopub.execute_input":"2022-03-04T20:04:51.640041Z","iopub.status.idle":"2022-03-04T20:04:53.087975Z","shell.execute_reply.started":"2022-03-04T20:04:51.639934Z","shell.execute_reply":"2022-03-04T20:04:53.087144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport librosa\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport soundfile as sf\nfrom joblib import Parallel, delayed\nfrom pathlib import Path\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:04:53.089861Z","iopub.execute_input":"2022-03-04T20:04:53.090168Z","iopub.status.idle":"2022-03-04T20:04:59.833745Z","shell.execute_reply.started":"2022-03-04T20:04:53.090118Z","shell.execute_reply":"2022-03-04T20:04:59.83283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nDATA_PATH = \"../input/birdclef-2022/\"\nAUDIO_PATH = '../input/birdclef-2022/train_audio'\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\nNUM_WORKERS = 4\nCLASSES = sorted(os.listdir(AUDIO_PATH))\nNUM_CLASSES = len(CLASSES)\n\nclass AudioParams:\n    \"\"\"\n    Parameters used for the audio data\n    \"\"\"\n    sr = 32000\n    duration = 5\n    # Melspectrogram\n    n_mels = 224\n    fmin = 20\n    fmax = 16000\n\ntrain = pd.read_csv('../input/birdclef-2022/train_metadata.csv')\ntrain[\"file_path\"] = AUDIO_PATH + '/' + train['filename']\npaths = train[\"file_path\"].values\n\nFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nfor n, (trn_index, val_index) in enumerate(Fold.split(train, train['primary_label'])):\n    train.loc[val_index, 'kfold'] = int(n)\ntrain['kfold'] = train['kfold'].astype(int)\n\ntrain.to_csv('train_folds.csv', index=False)\n\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:04:59.835593Z","iopub.execute_input":"2022-03-04T20:04:59.835915Z","iopub.status.idle":"2022-03-04T20:05:00.201363Z","shell.execute_reply.started":"2022-03-04T20:04:59.835873Z","shell.execute_reply":"2022-03-04T20:05:00.200602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_melspec(y, params):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = librosa.feature.melspectrogram(\n        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n    )\n\n    melspec = librosa.power_to_db(melspec).astype(np.float32)\n    return melspec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_or_pad(y, length, sr, train=True, probs=None):\n    \"\"\"\n    Crops an array to a chosen length\n    Arguments:\n        y {1D np array} -- Array to crop\n        length {int} -- Length of the crop\n        sr {int} -- Sampling rate\n    Keyword Arguments:\n        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n    Returns:\n        1D np array -- Cropped array\n    \"\"\"\n    if len(y) <= length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n    else:\n        if not train:\n            start = 0\n        elif probs is None:\n            start = np.random.randint(len(y) - length)\n        else:\n            start = (\n                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n            )\n            start = int(sr * (start))\n\n        y = y[start: start + length]\n\n    return y.astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mono_to_color(X, eps=1e-6, mean=None, std=None):\n    \"\"\"\n    Converts a one channel array to a 3 channel one in [0, 255]\n    Arguments:\n        X {numpy array [H x W]} -- 2D array to convert\n    Keyword Arguments:\n        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n        mean {None or np array} -- Mean for normalization (default: {None})\n        std {None or np array} -- Std for normalization (default: {None})\n    Returns:\n        numpy array [3 x H x W] -- RGB numpy array\n    \"\"\"\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# original\n\npath = train['file_path'][0]\ny, sr = sf.read(path, always_2d=True)\ny = np.mean(y, 1)\n\nX = compute_melspec(y, AudioParams)\nX = mono_to_color(X)\nX = X.astype(np.uint8)\n\nplt.imshow(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5 sec cropped\n\npath = train['file_path'][0]\ny, sr = sf.read(path, always_2d=True)\ny = np.mean(y, 1)\ny = crop_or_pad(y, AudioParams.duration * AudioParams.sr, sr=AudioParams.sr, train=True, probs=None)\n\nX = compute_melspec(y, AudioParams)\nX = mono_to_color(X)\nX = X.astype(np.uint8)\n\nplt.imshow(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Audio_to_Image(path, params):\n    y, sr = sf.read(path, always_2d=True)\n    y = np.mean(y, 1) # there is (X, 2) array\n    y = crop_or_pad(y, params.duration * params.sr, sr=params.sr, train=True, probs=None)\n    image = compute_melspec(y, params)\n    image = mono_to_color(image)\n    image = image.astype(np.uint8)\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_(path):\n    save_path = \"../working/\" + \"/\".join(path.split('/')[-2:])\n    np.save(save_path, Audio_to_Image(path, AudioParams))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parallel Execution\nNUM_WORKERS = 4\nfor dir_ in CLASSES:\n    _ = os.makedirs(dir_, exist_ok=True)\n_ = Parallel(n_jobs=NUM_WORKERS)(delayed(save_)(AUDIO_PATH) for AUDIO_PATH in tqdm(paths))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nmydict = {\n    \"label\": [],\n    \"segment\": []\n}\n\nfor folder in CLASSES:\n    for file in os.listdir(folder):\n        mydict[\"label\"].append(folder)\n        segment = np.load(os.path.join(folder, file))\n        mydict[\"segment\"].append(segment)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nlabels = []\n\nfor folder in CLASSES:\n    for file in os.listdir(folder):\n        labels.append(folder)\n        segment = np.load(os.path.join(folder, file))\n        data.append(segment)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:05:02.332368Z","iopub.execute_input":"2022-03-04T20:05:02.332646Z","iopub.status.idle":"2022-03-04T20:05:29.615048Z","shell.execute_reply.started":"2022-03-04T20:05:02.332616Z","shell.execute_reply":"2022-03-04T20:05:29.614232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(data)\ntargets = np.array(labels)\ny = pd.get_dummies(targets).values","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:05:29.617297Z","iopub.execute_input":"2022-03-04T20:05:29.617844Z","iopub.status.idle":"2022-03-04T20:05:30.570843Z","shell.execute_reply.started":"2022-03-04T20:05:29.6178Z","shell.execute_reply":"2022-03-04T20:05:30.570015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_val,y_train,y_val = train_test_split(X,y,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:05:30.572063Z","iopub.execute_input":"2022-03-04T20:05:30.572339Z","iopub.status.idle":"2022-03-04T20:05:32.340319Z","shell.execute_reply.started":"2022-03-04T20:05:30.572303Z","shell.execute_reply":"2022-03-04T20:05:32.339431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def design_model(input_shape):\n\n    # load the InceptionResNetV2 architecture with imagenet weights as base\n    base_model = tf.keras.applications.InceptionResNetV2(\n                         include_top=False,\n                         weights='imagenet',\n                         input_shape=input_shape\n                         )\n\n    base_model.trainable=False\n    # For freezing the layer we make use of layer.trainable = False\n    # means that its internal state will not change during training.\n    # model's trainable weights will not be updated during fit(),\n    # and also its state updates will not run.\n\n    model = tf.keras.Sequential([ \n            base_model,   \n            tf.keras.layers.BatchNormalization(renorm=True),\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(512, activation='relu'),\n            tf.keras.layers.Dense(256, activation='relu'),\n            tf.keras.layers.Dropout(0.5),\n            tf.keras.layers.Dense(128, activation='relu'),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n            ])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:07:06.662149Z","iopub.execute_input":"2022-03-04T20:07:06.662783Z","iopub.status.idle":"2022-03-04T20:07:06.670131Z","shell.execute_reply.started":"2022-03-04T20:07:06.662741Z","shell.execute_reply":"2022-03-04T20:07:06.669066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\ninput_shape = (X_train.shape[1], X_train.shape[2], 3)\nmodel = design_model(input_shape)\n\n# Selection of the optimizer, loss type and metrics for performance evaluation.\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy', tfa.metrics.F1Score(num_classes=len(train.primary_label.unique()))]\n             )\n\n\nmodel.summary()\n\n# Training the model.\nhistory = model.fit(X_train, y_train,\n                    validation_data=(X_val, y_val),\n                    epochs=3,\n                    batch_size=8\n                    )\n\n#plot_performance(history)\n\n# Testing the model on never seen before data.\n#make_prediction(model, Xtest, ytest, 24)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T20:07:07.713616Z","iopub.execute_input":"2022-03-04T20:07:07.713899Z"},"trusted":true},"execution_count":null,"outputs":[]}]}